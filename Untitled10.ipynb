{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import re\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"NewsClassification\").getOrCreate()\n",
        "\n",
        "# Load the datasets into Spark DataFrames\n",
        "true_df = spark.read.csv('/content/True.csv', header=True, inferSchema=True)\n",
        "fake_df = spark.read.csv('/content/Fake.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Add label columns\n",
        "true_df = true_df.withColumn('label', lit(1))\n",
        "fake_df = fake_df.withColumn('label', lit(0))\n",
        "\n",
        "# Combine the sub-datasets into one\n",
        "news_df = true_df.union(fake_df)\n",
        "#randomize the news_df then take only the top 200\n",
        "news_df = news_df.orderBy(rand())\n",
        "news_df = news_df.limit(200)\n",
        "\n",
        "# Define a function for text preprocessing using NLTK\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "FPyfaBDdzV5V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f376f3a-8cae-4230-84e2-061e9da3ed52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)  # Remove non-alphabetic characters\n",
        "    text = text.lower().split()  # Convert to lowercase and split into words\n",
        "    text = [ps.stem(word) for word in text if word not in stop_words]  # Stemming and remove stopwords\n",
        "    return ' '.join(text)"
      ],
      "metadata": {
        "id": "6I4tBySfzV9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply text preprocessing to the 'text' column\n",
        "preprocess_udf = udf(preprocess_text, StringType())\n",
        "news_df = news_df.withColumn('processed_text', preprocess_udf(col('text')))"
      ],
      "metadata": {
        "id": "hk72umtPzV_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorization using Spark MLlib\n",
        "tokenizer = Tokenizer(inputCol='processed_text', outputCol='words')\n",
        "remover = StopWordsRemover(inputCol='words', outputCol='filtered_words')\n",
        "hashing_tf = HashingTF(inputCol='filtered_words', outputCol='raw_features', numFeatures=5000)\n",
        "idf = IDF(inputCol='raw_features', outputCol='features')\n"
      ],
      "metadata": {
        "id": "mebBU8LnAPPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(stages=[tokenizer, remover, hashing_tf, idf])\n",
        "pipeline_model = pipeline.fit(news_df)\n",
        "news_df = pipeline_model.transform(news_df)\n",
        "\n",
        "# Select the features and label columns\n",
        "news_df = news_df.select('features', 'label')"
      ],
      "metadata": {
        "id": "h6ajgYslAPRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and test sets\n",
        "train_df, test_df = news_df.randomSplit([0.8, 0.2], seed=0)\n",
        "\n",
        "# Train a logistic regression model\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
        "lr_model = lr.fit(train_df)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = lr_model.transform(test_df)\n",
        "\n",
        "# Evaluate the model using BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator()\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Accuracy: {accuracy *100}\")\n",
        "\n",
        "# Optionally, print more evaluation metrics\n",
        "predictions.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McjnOvt5APT7",
        "outputId": "22edb087-c03b-4cc1-ee9a-ac1972eb3b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 94.23076923076923\n",
            "+--------------------+-----+--------------------+--------------------+----------+\n",
            "|            features|label|       rawPrediction|         probability|prediction|\n",
            "+--------------------+-----+--------------------+--------------------+----------+\n",
            "|(5000,[6,88,95,98...|    1|[-0.9061859304392...|[0.28778094747659...|       1.0|\n",
            "|(5000,[6,122,158,...|    0|[0.32763983290708...|[0.58118500126319...|       0.0|\n",
            "|(5000,[7,19,132,1...|    0|[0.32763983290708...|[0.58118500126319...|       0.0|\n",
            "|(5000,[7,29,51,64...|    1|[-0.9061859304392...|[0.28778094747659...|       1.0|\n",
            "|(5000,[7,63,89,12...|    1|[-0.9061859304392...|[0.28778094747659...|       1.0|\n",
            "|(5000,[8,30,33,41...|    0|[0.32763983290708...|[0.58118500126319...|       0.0|\n",
            "|(5000,[8,46,64,95...|    1|[-0.2892730487661...|[0.42818184607372...|       1.0|\n",
            "|(5000,[8,146,171,...|    1|[-0.9061859304392...|[0.28778094747659...|       1.0|\n",
            "|(5000,[15,29,87,9...|    1|[-0.9061859304392...|[0.28778094747659...|       1.0|\n",
            "|(5000,[15,52,64,1...|    1|[-0.2892730487661...|[0.42818184607372...|       1.0|\n",
            "|(5000,[16,88,89,1...|    0|[0.32763983290708...|[0.58118500126319...|       0.0|\n",
            "|(5000,[18,63,64,8...|    1|[-0.2892730487661...|[0.42818184607372...|       1.0|\n",
            "|(5000,[19,21,23,5...|    1|[-1.5230988121124...|[0.17900565674212...|       1.0|\n",
            "|(5000,[19,33,49,1...|    1|[-0.2892730487661...|[0.42818184607372...|       1.0|\n",
            "|(5000,[19,78,95,1...|    1|[-0.2892730487661...|[0.42818184607372...|       1.0|\n",
            "|(5000,[19,258,328...|    1|[-0.2892730487661...|[0.42818184607372...|       1.0|\n",
            "|(5000,[23,102,122...|    1|[-0.2892730487661...|[0.42818184607372...|       1.0|\n",
            "|(5000,[26,35,96,1...|    0|[0.32763983290708...|[0.58118500126319...|       0.0|\n",
            "|(5000,[26,55,88,1...|    0|[0.32763983290708...|[0.58118500126319...|       0.0|\n",
            "|(5000,[33,94,98,1...|    1|[-0.9061859304392...|[0.28778094747659...|       1.0|\n",
            "+--------------------+-----+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W40-PprKCgkY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}